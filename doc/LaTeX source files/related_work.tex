Problem instances similar to the one mentioned during the introduction were treated by papers including \cite{aupetit2009nearly} and \cite{klawonn2006equi}.

In the past years, a lot of work was done in order to develop systems that deal with big data. The goal was to make systems that can scale horizontally (i.e. easily can add commodity hardware to the system). The first approach proposed was MapReduce (introduced by Google in 2004 in the paper \cite{dean2004mapreduce}). The main idea was to distribute data over multiple nodes in a cluster(s) and assign transformation (map) and action (reduce) tasks to each node (i.e. moving the operations to the data). This idea is implemented in the Hadoop framework. The downside the Hadoop MapReduce approach had was that each time we perform a transformation or an action over a part of the data in a node, we read from the disk and write the results on the disk. The disk operations are expensive. Therefore, another framework called \href{https://spark.apache.org/}{SPARK} (started at UC Berkeley's AMPLab in 2009) was developed in order to make data analysis more efficient. The main idea of the SPARK framework is to store the data in a distributed manner over the nodes' main memory. In such a way the costly read/write operations are sped up. Moreover, the SPARK ecosystem allows for more operations than simply mapping and reducing. This framework was developed with machine learning algorithms in mind from the start. SPARK ecosystem provides efficient implementations of different clustering algorithms. Power Iterative Clustering (PIC) \cite{PIC} is one of them. In our paper, we use the SPARK framework in order to implement our proposed algorithms to scale well on multiple machines and we use PIC (SPARK implementation) as a baseline for our problem for the reasons we present in section \ref{Section:Baseline}.