In the era of digitisation, information became the new gold. The amount of data produced increases each day, especially due to the internet where users themselves produce more data and automated machine systems produce a considerable amount of data (e.g. sensor readings). The need to store and analyse this large amount of data resulted in the apparition of big data, which is characterised by four V's: Volume, Variety, Velocity and Veracity.
Systems like HDFS attempt to tackle the efficient storing and retrieval of the data in a distributed manner. While frameworks like \href{https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html}{Hadoop MapReduce} and \href{https://spark.apache.org/}{SPARK} are used to efficiently query this information to acquire insight. In this paper, we will consider algorithms that run on top of SPARK to cluster data from a structured table consisting of records (i.e. tuples).

\paragraph{Problem Statement}
Given a data set $D$, we want to return a decomposition of $D$ into $k$ (potentially overlapping) subsets $D_1, D_2, \dots , D_k$ such that each subset is as homogeneous (i.e. has a low variety) as possible. We call such a subset $D_i \subseteq D$ an \textit{area} or \textit{cluster}. The average homogeneity of these subsets is to be maximised. It is obvious that an area consisting of only one record has the highest homogeneity possible. However, we are interested in such areas that are insightful, so the results can be further exploited. To formalise this statement, we consider only the areas that can be expressed in a declarative way as a Conjunctive Query (CQ) (i.e. of the form $attr_1=value_1 \ AND \ attr_2=value_2 \ AND \ \dots \ AND \ attr_m=value_m$). For instance, the query: $firstname="Marry" \ AND \ city="Utrecht"$ describes the area that consists of all the records that have the first name Marry and the city Utrecht. Since the areas are representing a decomposition of $D$, we want that $D_1 \cup D_2 \cup ... \cup D_k = D$. In essence, this problem is a generalisation of the clustering problem.

\paragraph{Motivation}
The problem of partitioning the data into homogeneous areas is of high significance, especially in the context of large amounts of data. Suppose we want to train a machine learning model using our data set. Training a model over the complete data set is infeasible for a humongous amount of records. The solution is to train an ensemble of models, each on a portion of the data and consider their aggregate decisions. This way, we can train them in parallel. This parallelism makes it possible to train the models faster. However, think about what portion of data is better for a model. If we feed each model with sets of high variety, the models will hardly capture the structure of the complete data set. On the other hand, if we use a highly homogeneous subset of the data for each model, each will capture the characteristics of the entity(es) described by that particular portion, thus will perform better in discriminating new instances. Therefore, partitioning the data into homogeneous subsets, not only enables ML algorithms to be applied in the context of big data but also increases the accuracy of these models. Moreover, working with big data means working with distributed data along nodes in one or more clusters. The time for querying such a distributed data set depends on the query itself and the physical partitioning of the data. The ideal partition should achieve optimal response times for each query. Partitioning data into homogeneous sets that can be expressed with a CQ results in very efficient filtering queries. For instance, suppose we want to select the records that have in the column age a number bigger than 18, then we can ignore the areas that have in their CQ, $age = v$ with $v < 18$. 

\paragraph{The nature of the problem and possible solutions}
Let us denote the number of the records in a data set $D$ with $N$. The number of subsets of a set of $N$ items is $2^N$. The number of sets of such subsets is $2^{2^N}$. This means, that we can have $2^{2^N}$ possible decompositions of the data set. Of course, due to the restrictions imposed for such a decomposition, the number of possible solutions is smaller. Suppose we have $m$ columns in our data set, each with $n$ unique values.
We can construct $(n+1)^{m}$ conjunctive queries. To partition the data set into $k$ subsets we have ${(n+1)^m \choose k}$ possibilities. Restriction of having a collection that covers the data set reduces the number of possibilities. However, notice the exponential nature of the problem. In the context of big data the exponential complexity problems are infeasible to solve, thus, so is the exhaustive search for the solution to this problem. As we will see in this paper, even polynomial time complexity is a problem. We would like to have an algorithm that solves this problem in linear or even sub-linear time. In our work, we propose a greedy approach that uses the advantages of distributed data systems to solve this problem. We compare the results of our algorithm with the ones from an existing clustering algorithm implementation. 

The remaining of this paper is structured as follows. In section \ref{Section:RelatedWork} we discuss the related works present in the literature along with important concepts and implementations of distributed data systems. Section \ref{Section:Homogeneity}, formalises the notion of homogeneity and proposes appropriate measures for it. In the following section \ref{Section:CustomTechnique} our new greedy approach is presented along with two variations. In section \ref{Section:ExperimentsResults} we present a set of empirical results and the experimental setup in order to compare our method with the baseline solution given by a clustering algorithm. We discuss our results in section \ref{Section:Discussions}. In the last section \ref{Section:Conclusion} we conclude our work and give directions for future works.