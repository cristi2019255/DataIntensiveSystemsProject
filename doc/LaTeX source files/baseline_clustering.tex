A basic approach one can think about in order to split the data into areas is a classical clustering approach like K-means or Dbscan. The problem is that these approaches work with real number vectors, and distance in Euclidean space. We want a method that works for any type of item in the data table. Without any prior knowledge about the data, we assume that our data records are tuples of strings. In order to say how similar are two tuples, we can compute the edit distance (i.e. Levenshtein distance) between each individual element of the first tuple with the corresponding element from the second tuple and sum up the distances in order to compute the distance between the tuples. In other words, we consider as the distance the total number of transformations we need to apply to the first tuple in order to get the second one. The edit distance is a commonly used technique to determine the similarity between two strings and thus makes a good fit for a clustering algorithm. For instance, consider the following tuples: $t_1 = ("Johnny", "Doe")$ and $t_2 = ("Joey", "D")$, to transform "Johnny" into "Joey" we need to transform the letter "h" into "e" and delete the letter "n" two times, to transform "Doe" to "D" we need to remove two letters. Thus, the number of transformations we need to apply is $1 + 2 + 2 = 5$. Of course, this distance will not perform very good for columns that contain numbers because, for instance, to transform "109" to "209" we need only one transformation and two transformations for "109" to "110", however, the first two are much more far away from each other. Moreover, even between strings, certain measures might be a better fit. For instance, the gap distance if a string represents an abbreviation of another. However, introducing such distance measures inject knowledge about the data, which we can not assume if we want a general approach. Therefore, edit distance is our choice as it assumes less about the data. We compute the Levenshtein distance between any pair of individual records $i$ and $j$ and place the distance $d(i,j)$ in a pair-wise similarity matrix. The downside of this approach is that we need $\frac{N (N - 1)}{2}$ operations to compute this matrix, with $N$ the number of data records. 

The clustering algorithm we use as a baseline is the Power Iteration Clustering (PIC) algorithm. Given the similarity matrix, this algorithm finds clusters within our data set by clustering (using the K-means algorithm) on the eigenvector of this matrix (see \cite{PIC}). In our experiments, we use the SPARK implementation of PIC. The matrix multiplication operations needed for finding the eigenvector are done in parallel as well as the clustering on the eigenvector. Even though the PIC is implemented in an efficient way and works in parallel, the complexity of the approach using PIC is quadratic due to having to determine the similarity matrix.