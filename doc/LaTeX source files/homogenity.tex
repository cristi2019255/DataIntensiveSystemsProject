In order to test and compare the approaches we first need a measure of homogeneity of a cluster. A possible homogeneity indication can be the amount of unique values, namely a cluster that has less unique values can be considered more homogeneous than one that has more. Suppose we have a data set consisting of only one column (i.e. each data record is a tuple consisting of a single value) with $N$ unique values, a partition into two areas $D_1$ and $D_2$ will give us $n_1$ unique values for $D_1$ and $n_2 = N - n_1$ for $D_2$. If $n_1 > n_2$ then we have more unique values in $D_1$ which means that $D_2$ is of higher homogeneity than $D_1$. One can think of computing the homogeneity of an one column cluster $X$ as $hom(X) = \frac{1}{unique(X)}$. If we consider the case where we have multiple columns we can simply consider the average homogeneity over each column. That is, if we have a cluster $X = (X_1, ..., X_m)$, where $X_i$ is a column then the homogeneity of that cluster can be considered as $hom(X) = \frac{1}{m} \sum_{i = 1}^m hom(X_i)$. However, this measure is not as good as it might seem. Suppose we have $X = (X_1, X_2)$, where $X \subset D$ and let us denote the number of unique values in the columns of $X$ with $n_1$ and $n_2$ and for $D$ with $N_1$ and $N_2$ respectively. If $n_1 < n_2$ the column $X_1$ will have a bigger influence on the homogeneity of $X$ regardless of $N_1$ and $N_2$. However it might happen that $n_2 \ll N_2$ and $n_1$ is almost equal to  $N_1$, in this case $X_2$ should mostly determine the homogeneity of $X$. Therefore, a better way is to compute the homogeneity of $X_1$ and $X_2$ as $1 - \frac{n_1}{N_1}$ and $1 - \frac{n_2}{N_2}$ respectively. That is, if data set $D$ has $m$ columns and for each column we have $N_i$ unique values and an area $X \subseteq D$ with $n_i, \forall i \in \{1, \dots, m\}$ unique values, the homogeneity of $X$ is: $hom_D(X) = 1 - \frac{1}{m}\sum_{i = 1}^m \frac{n_i}{N_i}$. Notice that $hom_D(D) = 0$. This measure is taking the data set as a reference point and computes the "increase in homogeneity" of an area compared to the data set. This easy to compute measure keeps track only of how many unique values we have in each column. However, consider two one-column areas where the number of unique values is $n_X = 2$ and $n_Y = 3$ respectively ($N = 5$). Given our measure, the area with two unique values has a higher homogeneity. What if the first area has 50 items of value $v_1$ and 50 items of value $v_2$ whilst the second area has 98 items of value $v'_1$ and only one item of $v'_2$ and one of $v'_3$. It feels wrong that the first area is preferred. In the second area the records are more related to each other. To fix this issue we propose a homogeneity measure that takes into account not only the unique values, but also their number of appearances in the column. We can regard a column $X$ with $N$ items as a random variable, where the distribution is given by $\mathbb{P}(x \in X) = \frac{n}{N}$, where $n$ is the number of items with value $x$. Using this distribution we can measure the uncertainty of $X$ using the entropy function: $H(X) =  - \sum_{x \in X} \mathbb{P}(x) \log \mathbb{P}(x)$, where $H(X) \in [0, \log N]$. If the entropy is low we are more certain about the values of $X$, thus we say the homogeneity is high. The homogeneity measure based on entropy is: \[\scalebox{1.2}{$hom_D(X) = 1 - \frac{1}{m} \sum\limits_{i=1}^m H_{norm}(X_i, N_i)$}\]. Where, $X_i$ is a column of an area $X$ with $m$ columns, the $N_i$ is the number of distinct values of column $i$ in the complete data set $D$ and $H_{norm}(X, N) = \left\{
\begin{array}{ll} 
    \frac{H(X)}{log N}, \ N \neq 1 \\
    0, \ N = 1
\end{array} 
\right.$.
We take the average of the normalised entropy for each column and favour the areas where this value is low. For example, if we were to have the data set from table \ref{tab:ExampleTable}
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
ID & Title     & Release \\ \hline
1  & Shrek     & 2001    \\ \hline
2  & Shrek     & 2004    \\ \hline
3  & Shrek     & 2007    \\ \hline
4  & Bee Movie & 2007    \\ \hline
5  & MegaMind  & 2010    \\ \hline
\end{tabular}
\caption{Example table}
\label{tab:ExampleTable}
\end{table}

we can calculate the average homogeneity by first calculating the homogeneity of the title column as \[ \scalebox{1.2}{$1-\frac{-\frac{3}{5}\log\frac{3}{5}-\frac{1}{5}\log\frac{1}{5}-\frac{1}{5}\log\frac{1}{5}}{\log 3}=0.135$}\] and that of the release column as
\[ \scalebox{1.2}{$1-\frac{-\frac{1}{5}\log\frac{1}{5}-\frac{2}{5}\log\frac{2}{5}-\frac{1}{5}\log\frac{1}{5}-\frac{1}{5}\log\frac{1}{5}}{\log 4}=0.039$}\] and subsequently taking the average of both resulting in
\[ \scalebox{1.1}{$\frac{0.135+0.039}{2}=0.087$} \]

Turning back to our previous example, when we have an one-column area $X$ that has 50 items of value $v_1$ and 50 items of value $v_2$ and a second area $Y$ has 98 items of value $v'_1$ and only one item of $v'_2$ and one of $v'_3$. The homogeneity of $X$ is $hom_D(X) = 1 - \frac{ - \frac{100}{100} \log \frac{50}{100}}{\log 5} = 0.569$ and the homogeneity of $Y$ is $hom_D(Y) = 1 - \frac{- \frac{98}{100} \log \frac{98}{100} - \frac{2}{100} \log \frac{1}{100}}{\log 5} = 0.930$. Thus, even though $X$ has less unique values, $Y$ is preferred. The homogeneity of the data set in this case is $1 - \frac{- \frac{100}{200} \log \frac{50}{200} - \frac{100}{200} \log \frac{100}{200} - \frac{2}{200} \log \frac{1}{200}}{\log 5} = 0.321$. In our experiments we use this entropy based homogeneity measure, however our goal is to present an approaches that regardless of the measure give as results clusters that have a high average homogeneity. 