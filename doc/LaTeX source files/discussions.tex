One of the key benefits of using a framework like SPARK to perform calculations or algorithms for clustering is the possibility to scale out to multiple machines. But due to limitations, we were unable to perform experiments in a distributed (on multiple machines) fashion. This is something that could be expanded upon in additional research.

When looking back at table \ref{tab:scalability_comparison} some remarkable information can be seen when looking at the run-time for the greedy algorithm, it appears to be running faster for more records in the data set. This can either be due to the structure of the data itself, resulting in easier-to-compute clusters and thus a lower run-time. Or this can be a result of framework caching or the code being optimised after running the smaller data set. Additional experiments could be performed to rule out unexpected behaviour. 

The experiments were all performed to compare the designed greedy algorithm with a PIC baseline. Any other implementation of a clustering algorithm could have been chosen instead. A different baseline implementation would have resulted in different comparisons to the greedy versions. More clustering algorithms can be implemented in future research to compare the homogeneity and run-time performance against the greedy algorithm in a broader test.

Additionally, the figures \ref{fig:homogenity_dbpediaProfiles} to \ref{fig:runtime_syntheticFinance} show the results for a limited amount of records. Additional testing could be performed for a broader spectrum of records. This added dimension could result in more insights into the benefits of either algorithm.